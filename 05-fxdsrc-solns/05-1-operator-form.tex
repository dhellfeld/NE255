\documentclass[12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{setspace}
\onehalfspacing

\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}
\usepackage{amsmath}
\usepackage{graphicx} % for graphics files
\usepackage{tabu}

% Draw figures yourself
\usepackage{tikz} 

% writing elements
%\usepackage{mhchem}

\usepackage{paralist}

% The float package HAS to load before hyperref
\usepackage{float} % for psuedocode formatting
\usepackage{xspace}

% from Denovo Methods Manual
\usepackage{mathrsfs}
\usepackage[mathcal]{euscript}
\usepackage{color}
\usepackage{array}

\usepackage[pdftex]{hyperref}
\usepackage[parfill]{parskip}

% math syntax
\newcommand{\nth}{n\ensuremath{^{\text{th}}} }
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\Macro}{\ensuremath{\Sigma}}
\newcommand{\rvec}{\ensuremath{\vec{r}}}
\newcommand{\vecr}{\ensuremath{\vec{r}}}
\newcommand{\omvec}{\ensuremath{\hat{\Omega}}}
\newcommand{\vOmega}{\ensuremath{\hat{\Omega}}}
\newcommand{\even}{\ensuremath{\phi^g}}
\newcommand{\odd}{\ensuremath{\vartheta^g}}
\newcommand{\evenp}{\ensuremath{\phi^{g'}}}
\newcommand{\oddp}{\ensuremath{\vartheta^{g'}}}
\newcommand{\Sn}{\ensuremath{S_N} }
\newcommand{\Ye}[2]{\ensuremath{Y^e_{#1}(\vOmega_#2)}}
\newcommand{\Yo}[2]{\ensuremath{Y^o_{#1}(\vOmega_#2)}}
\newcommand{\sigg}[1]{\ensuremath{\Macro^{gg'}_{s\,#1}}}
\newcommand{\psig}{\ensuremath{\psi^g}}
\newcommand{\Di}{\ensuremath{\Delta_i}}
\newcommand{\Dj}{\ensuremath{\Delta_j}}
\newcommand{\Dk}{\ensuremath{\Delta_k}}
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
\begin{document}
\begin{center}
{\bf NE 255, Fa16 \\
Operator Form and Iteration Methods\\
October 20, 2016}
\end{center}

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

We have now fully discretized the transport equation: 
\begin{align}
\vOmega_a \cdot \nabla \psi^g_a(\vecr) &+ \Sigma^g_t(\vecr)\psi^g_a(\vecr) =   \label{eq:mg-sn-transport}\\
&\sum_{g'=1}^G
  \sum_{l=0}^N
  \sigg{l}(\vec{r})
  \Bigl[
  Y^e_{l0}(\vOmega_a)\evenp_{l0}(\vec{r}) +
  \sum_{m=1}^l
  \bigl(
  Y^e_{lm}(\vOmega_a)\evenp_{lm}(\vec{r}) +
  Y^o_{lm}(\vOmega_a)\oddp_{lm}(\vec{r})
  \bigr)\Bigr] + q_{a,e}^g(\vec{r})\:, \nonumber
\end{align}
\begin{alignat}{3}
  \even_{lm} &= \int_{4\pi}Y^e_{lm}(\vOmega)\psi^g(\vOmega)\:d\vOmega\:,
  \quad& m\ge 0\:,\label{eq:even-flux}\\
  %%
  \odd_{lm} &= \int_{4\pi}Y^o_{lm}(\vOmega)\psi^g(\vOmega)\:d\vOmega\:,
  \quad& m>0\:.\label{eq:odd-flux}
\end{alignat}
%
Now that we have these equations (+ spatial discretization) we're going to look at how we actually solve this with a computer. 

The  next set of material comes from\\
\hspace*{2em}R. N. Slaybaugh. \textit{Acceleration Methods for Massively Parallel Deterministic Transport}, a PhD Dissertation. University of Wisconsin, Madison, WI (2011).\\
\hspace*{2em}T. M. Evans, A. S. Stafford, R. N. Slaybaugh, and K. T. Clarno. ``Denovo -- New
Three-Dimensional Parallel Discrete Ordinates Code in SCALE." \textit{Nuclear Technology},
\textbf{volume 171}(2), pp. 171â€“200 (2010).

\section*{Operator Form}
We start by expressing the TE in operator notation; which makes it easier to talk about solution techniques. In general, uppercase bolded letters will indicate matrices and lowercase italicized letters will indicate vectors and scalars. The following operators are used to express the transport equation:\\
%
\hspace*{2em} $\mathbf{L} = \vOmega \cdot \nabla + \Macro_t$ is the transport operator, \\
\hspace*{2em} $\mathbf{M}$ is the operator that converts harmonic moments into discrete angles, \\
\hspace*{2em} $\mathbf{S}$ is the scattering matrix, \\
\hspace*{2em} $q_e$ contains the external source,\\
\hspace*{2em} $f$ contains the fission source, $\nu \Macro_{f}$; $\mathbf{F} =\mathbf{\chi} f^{T}$, \\ 
\hspace*{2em} $\mathbf{D} = \ve{M^{T}}\ve{W} = \sum_{a=1}^{n}Y^{e/o}_{lm}w_{a}$ is the discrete-to-moment operator. 

With this notation, \autoref{eq:mg-sn-transport} can be written as \autoref{eq:operator-form}; it can be formulated as an eigenvalue problem by replacing the fixed soruce term with the fission term, $\frac{1}{k}\mathbf{MF}\phi$. This has two unknowns, the angular flux and the moments, which are related by the discrete-to-moment operator as seen in Equation \eqref{eq:moments} [NOTE: $\phi$ are the flux moments, \textbf{not} the scalar flux].
%
\begin{align}
  \mathbf{L} \psi &= \mathbf{MS}\phi + \ve{M}q_{e} \label{eq:operator-form}\\
  \phi &= \mathbf{D}\psi 
  \label{eq:moments}
\end{align}
%The typical strategy for solving Equation \eqref{eq:operator-form} is to combine it with \eqref{eq:moments} and form one equation involving only the moments, where $Q$ is comprised of the fixed or fission source: 
%\begin{equation}
%   (\ve{I} - \ve{DL}^{-1}\ve{MS})\phi = Q \:.
%\end{equation}  
%This is solved for $\phi$ from which $\psi$ can be determined at the end of the calculation.
%
The size of the operators can be defined in terms of the granularity of discretization: \\
%
\hspace*{2em} $G$ = number of energy groups, \\
\hspace*{2em} $N$ = number of moments, \\
\hspace*{2em} $n$ = number of angular unknowns, \\
\hspace*{2em} $c$ = number of cells, \\
\hspace*{2em} $u$ = number of unknowns per cell, which is determined by spatial discretization. \\
%
These can be combined to define \\
\hspace*{2em} $\alpha = G \times n \times c \times u$ and \\
\hspace*{2em} $\beta = G \times N \times c \times u$. \\
Using $\alpha$ and $f$, Equation \eqref{eq:operator-form} can be presented in terms of operator size:\\
\[
(\alpha \times \alpha)(\alpha \times 1) = (\alpha \times \beta) (\beta \times \beta) (\beta \times 1) + (\alpha \times \beta) (\beta \times 1)\:.
%(\alpha \times \betaf) (\beta \times \beta) (\beta \times 1)\:.
\]
The index variables, their meaning, and their ranges are shown in Table \ref{table:index}. 
%
\begin{table}[!h]
\caption{Meaning and Range of Indices Used in Transport Discretization}
\begin{center}
\begin{tabular}{l c c c c}
\hline
Variable & Symbol & First & Last \\[0.5ex]
\hline
Energy & g & 1 & G \\
Solid Angle & a & 1 & n \\
Space & suppressed & n/a & n/a \\
Legendre moment ($P_{N}$) & $l$ & 0 & N \\
Spherical harmonic moment ($Y$) & m & 0 & $l$ \\
\hline
\end{tabular}
\end{center}
\label{table:index}
\end{table}
%

The structures of the vectors and matrices are shown in the next few equations as this can make the whole thing easier to understand and visualize. The angular flux vector is explicitly written first, where for each discrete angle, $a$, and energy group, $g$, the set of angular fluxes, $ \psi^g_a$, includes all spatial unknowns.
%
 \begin{align}
    \psi &=     \begin{pmatrix}
    [\psi]_{1} & [\psi]_2 & \cdots & [\psi]_g & \cdots [\psi]_{G} 
  \end{pmatrix}^T  \:, \quad \text{and}  \\
  %
    [\psi]_g &= \begin{pmatrix}
    \psi^g_1 & \psi^g_2& \cdots & \psi^g_a & \cdots \psi^g_n 
  \end{pmatrix}^T \:.  
\end{align}
%
\begin{alignat}{2}
  \mathbf{M} &=    \begin{pmatrix}
      [\ve{M}]_{11} & 0 & 0 & \cdots & 0 \\
      0 & [\ve{M}]_{22} & 0 & \cdots & 0 \\
      0 & 0 & [\ve{M}]_{33} & \cdots & 0 \\
      \vdots & \vdots & \vdots & \ddots   & \vdots \\
      0 & 0 & 0 & \cdots & [\ve{M}]_{GG} \\
    \end{pmatrix} \nonumber  \:,& \qquad
    %
  \mathbf{S}  =     \begin{pmatrix}
      [\ve{S}]_{11} & [\ve{S}]_{12} & [\ve{S}]_{13} & \cdots &
      [\ve{S}]_{1G} \\
      [\ve{S}]_{21} & [\ve{S}]_{22} & [\ve{S}]_{23} & \cdots &
      [\ve{S}]_{2G} \\
      [\ve{S}]_{31} & [\ve{S}]_{32} & [\ve{S}]_{33} & \cdots &
      [\ve{S}]_{3G} \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      [\ve{S}]_{G1} & [\ve{S}]_{G2} & [\ve{S}]_{G3} & \cdots &
      [\ve{S}]_{GG}
    \end{pmatrix} \nonumber  \:,
 \end{alignat}
 %
 \begin{alignat}{2}
      \mathbf{F}  &=     \begin{pmatrix}
     \chi^{1}[\nu\Macro_{f}]^{1} &\chi^{1}[\nu\Macro_{f}]^{2} & \cdots &
      \chi^{1}[\nu\Macro_{f}]^{G} \\
      \chi^{2}[\nu\Macro_{f}]^{1} &\chi^{2}[\nu\Macro_{f}]^{2} & \cdots &
      \chi^{2}[\nu\Macro_{f}]^{G}\\
      \vdots & \vdots & \ddots & \vdots \\
      \chi^{G}[\nu\Macro_{f}]^{1} &\chi^{G}[\nu\Macro_{f}]^{2} & \cdots &
      \chi^{G}[\nu\Macro_{f}]^{G}\\
    \end{pmatrix} \:, \nonumber & \qquad
    %
  [\ve{S}]_{gg'} = \begin{pmatrix}
    \sigg{0} & 0 & \cdots & 0  \\
    0 & \sigg{1} & \cdots & 0 \\
    \vdots & 0 & \ddots  & \vdots \\
     0 & 0 & \cdots & \sigg{N}
  \end{pmatrix}\:, \nonumber
 \end{alignat}
    %
\begin{equation}    
   [\ve{M}]_{gg} = \begin{pmatrix}
    \Ye{00}{1} & \Ye{10}{1} & \Yo{11}{1} & \Ye{11}{1} & 
    \Ye{20}{1} & \cdots & \Yo{NN}{1} & \Ye{NN}{1} \\
    \Ye{00}{2} & \Ye{10}{2} & \Yo{11}{2} & \Ye{11}{2} & 
    \Ye{20}{2} & \cdots & \Yo{NN}{2} & \Ye{NN}{2} \\
    \Ye{00}{3} & \Ye{10}{3} & \Yo{11}{3} & \Ye{11}{3} & 
    \Ye{20}{3} & \cdots & \Yo{NN}{3} & \Ye{NN}{3} \\
    \vdots     & \vdots     & \vdots     & \vdots     & 
    \vdots     &  \ddots    & \vdots     & \vdots     \\
    \Ye{00}{n} & \Ye{10}{n} & \Yo{11}{n} & \Ye{11}{n} & 
    \Ye{20}{n} & \cdots & \Yo{NN}{n} & \Ye{NN}{n}
  \end{pmatrix}\:. \nonumber \\
\end{equation}

\noindent Note that $[\ve{M}]_{11} = [\ve{M}]_{22} =  \hdots = [\ve{M}]_{GG} = [\ve{M}]$. These are written with subscripts to simplify the visualization of which blocks correspond to which equations and multiply which other blocks.

The lower
triangular part of $\ve{S}$ represents downscattering, the diagonal represents
in-group scattering, and the upper diagonal is upscattering. 

Finally, the
vector of flux moments for group $g$ has $t$ entries per spatial unknown, and
%
\begin{equation} 
[\phi]_g = \begin{pmatrix} \even_{00} & \even_{10} & \odd_{11}
& \even_{11} & \even_{20} & \cdots & \odd_{NN} & \even_{NN}
  \end{pmatrix}^T\:,
\end{equation} for each group.

Note also that the total size of $\ve{D}$ over all
groups and spatial unknowns is $(\beta \times \alpha)$ (where $\ve{W}$ is an $(n\times n)$ diagonal matrix of diagonal matrices of quadrature
weights).  Also, even though $\ve{M}$ maps
angular flux moments onto discrete angular fluxes, in general
$\psi\ne\ve{M}\phi$.  This constraint can be removed by using Galerkin
quadrature in which $\ve{D} = \ve{M}^{-1}$.


%--------------------------------------------------------------------------------
\section*{Solution Procedure}
Once the matrices are multiplied together, a series of single-group equations that are each only a function of space and angle result:
%
\begin{equation}
  \begin{aligned}
    \ve{L}[\psi]_1 &= [\ve{M}]\bigl([\ve{S}]_{11}[\phi]_1 + 
    [\ve{S}]_{12}[\phi]_2 + \ldots + [\ve{S}]_{1G}[\phi]_G\bigr) + 
    [\ve{M}][q_{e}]_1\:, \\
   % \frac{1}{k}[\ve{M}]\sum_{i=1}^{G}[\ve{F}]_{1i}[\phi]_{i} \:, \\
    %%
    \ve{L}[\psi]_2 &= [\ve{M}]\bigl([\ve{S}]_{21}[\phi]_1 + 
    [\ve{S}]_{22}[\phi]_2 + \ldots + [\ve{S}]_{2G}[\phi]_G\bigr) + 
    [\ve{M}][q_{e}]_2\:, \\
    %\frac{1}{k}[\ve{M}]\sum_{i=1}^{G}[\ve{F}]_{2i}[\phi]_{i} \:, \\
    %%
    &\vdots\\
    %%
    \ve{L}[\psi]_G &= [\ve{M}]\bigl([\ve{S}]_{G1}[\phi]_1 + 
    [\ve{S}]_{G2}[\phi]_2 + \ldots + [\ve{S}]_{GG}[\phi]_G\bigr) + 
       [\ve{M}][q_{e}]_G\:, \\
    % \frac{1}{k}[\ve{M}]\sum_{i=1}^{G}[\ve{F}]_{Gi}[\phi]_{i} \:.
  \end{aligned}
  \label{eq:group-equations}
\end{equation}
%
Each \textbf{within-group} equation is solved for that group's flux. 

If the groups are coupled together through upscattering, as they often are, then multiple \textbf{multigroup} solves over the coupled portion of the energy range may be required. 

If there is fission and the eigenvalue is desired, an additional \textbf{eigenvalue} solve is needed as well. 

Each of these levels of iteration typically uses a completely different solver. 

Note that the typical strategy for solving Equation \eqref{eq:operator-form} is to combine it with \eqref{eq:moments} and form one equation involving only the moments, where $Q = \ve{DL}^{-1}\ve{M}q_e$ is comprised of the fixed (or fission) source: 
\begin{equation}
   (\ve{I} - \ve{DL}^{-1}\ve{MS})\phi = Q \:.
\end{equation}  
This is solved for $\phi$ from which $\psi$ can be determined at the end of the calculation. This format looks like what we're used to seeing in numerical linear algebra: $\ve{A}x = b$.

\subsection*{Solver Basics}
There are two categories of methods for solving $\ve{A}x = b$, \textbf{direct} and \textbf{iterative}. 

\underline{Direct methods} solve the problem by inverting $\ve{A}$ and setting $x = \ve{A}^{-1}b$. If $\ve{A}$ is invertible this can be done explicitly. $\ve{A}$ is often not invertible, so most direct methods are based on factoring the coefficient matrix $\ve{A}$ into matrices that are easy to invert. The problem is then solved in pieces where each factored matrix is inverted to get the final solution. An example where this is done is LU factorization. These methods are often robust and require a predictable amount of time and storage resources. However, direct methods scale poorly with problem size, becoming increasingly expensive as problems grow large.% \cite{Benzi2002}.

\underline{Iterative methods} compute a sequence of increasingly accurate approximations to the solution. They generally require less storage and take fewer operations than direct methods, though they may not be as reliable. Iterative methods are highly advantageous for large problems because direct methods become intractable for systems of the size of those of interest here. For this reason the nuclear energy industry tends to use iterative methods for transport calculations.% \cite{Birkhoff1984}, \cite{Benzi2002}. 

Within the category of iterative methods, two further subdivisions can be made that will be useful in this work. Some methods only place data from previous iterations on the right hand side of the equation. The order in which those equations are solved is then irrelevant because only the previous iterate is needed. The other class of methods place data from both the previous and current iteration on the right hand side. These methods are fundamentally sequential and must be solved in order. These two categories will be referred to as \textit{order independent} and \textit{order dependent}, respectively. 

%---------------------------------------------------
%---------------------------------------------------
\subsection*{Within Group Iterative Methods}
The methods that functionally perform a mesh sweep
\[
[\psi]_g = \ve{L}^{-1}\bigl([\ve{M}]\bigl([\ve{S}]_{11}[\phi]_1 + 
    [\ve{S}]_{12}[\phi]_2 + \ldots + [\ve{S}]_{1G}[\phi]_G\bigr) + 
    [\ve{M}][q_{e}]_1\bigr)\:.
\]
%---------------------------------------------------
\subsubsection*{Richardson iteration}
The simplest iteration scheme used by the nuclear community is source iteration (SI), also known as \textbf{Richardson iteration}. SI is applied to the within-group space-angle iterations. Richardson iteration can be thought of as a two-part process for the neutron transport equation, where $\bar{Q}$ includes all sources and $k$ is the inner iteration index:
%
\begin{align}
  \ve{L}[\psi]_g^{k+1} &= \ve{MS} [\phi]_g^k + [\bar{Q}]_{g} \:,   \label{eq:SIpsi} \\
  [\phi]_g^{k+1} &= \ve{D}[\psi]_g^{k+1} \:.
  \label{eq:SIphi}
\end{align}
%
The spectral radius, $c = \Macro_s / \Macro_t$, determines the speed of convergence. For problems dominated by scattering, SI will converge very slowly (good explanation of how to think of this physically on p.\ 2, 3 of  \href{https://github.com/rachelslaybaugh/NE250/blob/master/23-iterations/Nov-30Class.tex}{https://github.com/rachelslaybaugh/NE250/blob/master/23-iterations/Nov-30Class.tex}, which you can build using pdflatex). 

%---------------------------------------------------
\subsubsection*{Krylov methods}
\textbf{Krylov methods} are a powerful class of subspace methods that can be ideal for solving various types of linear and eigenvalue problems. A Krylov method solves $\ve{A}x = b$ by building a solution from a Krylov subspace generated by an iteration vector $v_{1}$. At iteration $k$, the subspace is:
%
\begin{equation}
  \mathcal{K}_{k}(\ve{A},v_{1}) \equiv span\{v_{1}, \ve{A}v_{1}, \ve{A}^{2}v_{1}, ..., \ve{A}^{k-1}v_{1}\} \:.
  \label{eq:Krylov-subspace}
\end{equation}
%
The choice of $v_{1}$ varies, but $v_{1} = b$ is common. %When the problem is presented as $\ve{A}z = r_{0}$ where $r_{0} \equiv b - \ve{A}x_{0}$ and the solution is $x_{k} = x_{0} + z$, then $v_{1} $ is often chosen to be $r_{0}$. Note that $x_{0}$ is the initial guess and $x_{k}$ is the solution approximation at step $k$.%  \cite{Ipsen1998}.

The dimension of a Krylov space is bounded by $n$ because Krylov methods will give the exact solution after $n$ iterations in the absence of roundoff error. Interestingly, this technically makes Krylov methods a hybrid of direct and iterative methods because an exact answer can be obtained in a predetermined number of steps. Krylov subspace methods are nevertheless generally classified as iterative methods.% \cite{Birkhoff1984}.

Krylov methods are particularly useful in a few pertinent cases. One is when $\ve{A}$ is very large because fewer operations are required than traditional inversion methods like Gaussian elimination. Another is when $\ve{A}$ is not explicitly formed because Krylov methods only need the action of $\ve{A}$. Finally, Krylov methods are ideal when $\ve{A}$ is sparse because the number of operations are low for each matrix-vector multiplication. For deterministic transport codes, $\ve{A}$ is typically quite large, fairly sparse, and only its action is needed. The action of $\ve{A}$ is implemented through the transport sweeps.% \cite{Lewis1993}, \cite{Ipsen1998}.  

In the last few decades Krylov methods have been used widely to solve problems with appropriate properties for several reasons. Krylov methods are robust; the existence and uniqueness of the solution can be established; typically far fewer than $n$ iterations are needed when they are used as iterative solvers; they can be preconditioned to significantly reduce time to solution; only matrix-vector products are required; explicit construction of intermediate residuals is not needed; and they have been found to be highly efficient in practice.% \cite{Ipsen1998}, \cite{Knoll2004}. 

There are, however, a few drawbacks. In some cases Krylov methods can be very slow to converge, causing large subspaces to be generated and thus becoming prohibitively expensive in terms of storage size and cost of computation. Some methods can be restarted after $m$ steps %\footnote{For information about restarted Krylov methods see Appendix \ref{sec:AppendixB}.} 
to alleviate this problem, keeping the maximum subspace size below $\mathcal{K}_{m+1}$.  The relatively inexpensive restart techniques can reduce the storage requirements and computational costs associated with a slow-converging problem such that they are tractable. Preconditioners can also help by reducing the number of iterations needed. %Development of appropriate preconditioners is an active area of research \cite{Warsa2004a}, \cite{Ipsen1998}, \cite{Knoll2004}. 

%In the 1970s interest in Krylov methods in the wider computational community began to increase after it was demonstrated that these methods can converge quickly. At first Krylov methods were restricted to problems where $\ve{A}$ is symmetric positive definite (SPD). These are methods such as conjugate gradient (CG), MINRES, SYMMLQ, and others. Then, Krylov methods for non-symmetric matrices became a focus, including the Generalized Minimum Residual (GMRES) and BiConjugate Gradient Stabilized (BiCGSTAB) methods \cite{Barrett1994}, \cite{Benzi2002}. The transport problem has characteristics that make it well suited for solution with Krylov methods, and these methods are used in several novel ways throughout this work.

See aside about Krylov Methods here: \href{ https://github.com/rachelslaybaugh/NE255/blob/gh-pages/05-fxdsrc-solns/05-2-krylov.pdf}{ https://github.com/rachelslaybaugh/NE255/blob/gh-pages/05-fxdsrc-solns/05-2-krylov.pdf} for more information.

\subsection*{Outer Iterations}
Once we have solved the space-angle component inside of a group, we need to loop over groups. There are several strategies for this, with Jacobi and Gauss Seidel being the most common (and multigroup Krylov becoming more common).

\subsubsection*{Jacobi}
The Jacobi method is known as the method of \textit{simultaneous displacements}. % \cite{LeVeque2007}. 
This could be applied as the outer iteration method over energy and can be written as follows, where $j$ is the outer iteration index:
%
\begin{equation}
  \ve{L} [\psi]_{g}^{j+1} = [\ve{M}] [\ve{S}]_{gg}[\phi]_{g}^{j} +   [\ve{M}]\bigl(\sum_{g'=1}^{g-1} [\ve{S}]_{gg'}[\phi]_{g'}^{j} + \sum_{g'=g+1}^{G} [\ve{S}]_{gg'}[\phi]_{g'}^{j} + [q_{e}]_{g} \bigr) \label{eq:Jacobi} \:. 
\end{equation}
%
The Jacobi method is order \textit{independent} since all terms on the right are at the old iteration level. Jacobi is unconditionally stable and linearly convergent, but may converge very slowly. The convergence of Jacobi is governed by its spectral radius.% \cite{LeVeque2007}. 

------------------------------------------------\\
\textbf{Aside:}\\
The spectrum of eigenvalues of a matrix $\ve{A}$ are defined formally as
\[\sigma(\ve{A}) = [ \lambda \in \mathbb{C} : \det(\ve{A} - \lambda \ve{I})=0] \] 
and an eigenvalue is 
\[ \lambda \in \sigma(\ve{A})\:.\]

The spectral radius of $\ve{A} \in \mathbb{C}^{n \times n}$ is defined as 
\[\rho(\ve{A}) \equiv \max \lbrace |\lambda|, \lambda \in \sigma(\ve{A}) \rbrace\]
------------------------------------------------

\subsubsection*{Gauss Seidel}
Gauss Seidel (GS) is known as the method of \textit{successive displacements}. % \cite{LeVeque2007}. 
It is commonly used as the outer iteration method over energy and can be written as follows:
%
\begin{equation}
  \ve{L} [\psi]_{g}^{j+1} = [\ve{M}] [\ve{S}]_{gg}[\phi]_{g}^{j+1} +   [\ve{M}]\bigl(\sum_{g'=1}^{g-1} [\ve{S}]_{gg'}[\phi]_{g'}^{j+1} + \sum_{g'=g+1}^{G} [\ve{S}]_{gg'}[\phi]_{g'}^{j} + [q_{e}]_{g} \bigr) \label{eq:GaussSeidel} \:. 
\end{equation}
%
The GS method is order \textit{dependent}, containing terms on the right hand side at both the new and old iterates. Once the space-angle iterations for each upscattering group are completed for one outer iteration, the right hand side is recalculated and the within-group calculations are repeated. This goes on until the entire system converges.% \cite{Evans2009d}. 

Gauss Seidel is also unconditionally stable and linearly convergent, but may converge very slowly. The convergence of GS is also governed by its spectral radius, $\rho$, though it converges twice as fast as the Jacobi method.% \cite{LeVeque2007}. 

The spectral radius of Gauss Seidel has been found for various materials by solving the eigenvalue problem $(\ve{T} - \ve{S}_{D})^{-1}\ve{S}_{U} \xi = \rho \xi$, where $\ve{T}$ is the matrix of total cross sections, $\ve{S}_{U}$ is the upper triangular portion of $\ve{S}$, $\ve{S}_{D}$ contains the diagonal and lower triangular portion, and $\xi$ is the eigenvector. This eigenvalue problem comes from Fourier analysis. The spectral radii for some materials of practical interest determined using cross sections with 41 thermal upscattering groups are: graphite = 0.9984, heavy water = 0.9998, and iron = 0.6120. % \cite{Adams2002}, \cite{Evans2009d}. 
Problems containing graphite or heavy water would converge very slowly if GS were used.   


\end{document}
